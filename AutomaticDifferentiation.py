#这里介绍一个autograd的包
#autograd包为Tensor上所有操作提供自动求导
#它是一个由运行定义的框架，这意味这以代码定义的方式定义反向传播，并且每次迭代都可以不同
#怎么理解这句话呢？

#首先：由运行定义的框架
#TensorFlow中需要提前定义好整个计算图，之后再运算
#而pytorch中在运行时动态构建计算图

#其次：以代码定义的方式定义反向传播
#在由运行定义的框架中，反向传播的过程是依靠前向传播时实际运行的代码来自动确定的
#这意味着只需要编写前向传播的代码，框架会根据这些代码自动记录每一步操作

#最后：每次迭代都可以不同
#由于计算图在运行时时动态构建的，所以每次迭代时，前向传播的代码逻辑可以不同，计算图也会随之改变。

#torch.Tensor是包的核心类
#如果将其属性.requires_grad设置为True，那么它将开始跟踪对于该张量的所有操作
#当完成计算后，可以调用.backward()来自动计算所有梯度
#这个张量的所有梯度将会自动累加到.grad属性中

